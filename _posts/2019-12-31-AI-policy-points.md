---
id: 10009
title: Draft AI Policy Points
facebookcomments: true
author: Dan Elton
layout: post
permalink: /2019/12/31/AI-policy-points.html
categories:
  - artificial intelligence
tags:
  - transhumanist party
  - artificial intelligence
  - policy
---

This is a **draft** of policy points I put together for the [US Transhumanist Party](https://en.wikipedia.org/wiki/Transhumanist_Party)'s nominee for president in 2020. 

# Problem 1  - mass unemployment from automation &amp; AI

Policy solutions:

- UBI or negative income tax to prevent poverty trap and allow people to get back on their feet. Fund by the federal land dividend and by shifting money out of defense.
- Invest in skills retraining programs

###  Subproblem 1.1. - Increasing rates of depression from loss of purpose, feeling of being an &quot;underclass&quot; relative to the &quot;tech elite&quot;, leading to suicide &amp; more opiod use.

- Encourage people to pursue creative work or volunteer.
- Focus more on mental health and wellbeing instead of GDP.


# Problem 2 - Lethal autonomous weapons

###  Subproblem 2.1 - Threat of expensive &amp; pointless AI arms race

Policy solutions:

- International treaties

###  Subproblem 2.2. - Threat of LAWS tech falling into hands of terrorist organizations and malicious state actors

Policy solutions:

- DoD research on countermeasures for drones and other autonomous attack vectors.
- Ethical use of surveillance technology to monitor threats while ensuring privacy

# Problem 3 - US needs talent to compete on AI for national defense and economic development

Policy solutions:

- Make DoD jobs more appealing to young people. People should not be forced to work on weapons systems, instead they obtain jobs which are limited to AI safety and defense-only systems in DoD.
- Make it easier for highly skilled individuals immigrate and/or stay in the US after PhDs or postdocs.
- Invest in education - coding should be a mandatory subject in all schools.

# Problem 4 (long term) - threat of AGI &quot;takeoff&quot; / &quot;singularity&quot; / &quot;paperclip maximizer&quot;

###  Subproblem 4.2 - threat of &quot;unipolar&quot; outcome where one country obtains &quot;AI supremacy&quot;. (ie China taking over the world with highly advanced AI)

Policy solutions:

- Programs to maintain national competitiveness and prevent the ongoing IP theft from China.
- Encourage Initiatives like SingularityNET and OpenAI to decentralize and distribute AI innovations widely, to prevent global domination by an AI &quot;singleton&quot;.

###  Subproblem 4.2 - near term AI safety

There is growing realization that current AI systems are susceptible to adversarial attacks and can be easily hacked.

Policy solutions:

- Invest in research on robustness, interpretability, and AI bias.
- Get more smart technical people involved in regulation of AI systems where human lives are at stake such as AI diagnostic systems or driverless cars. Currently most AI experts are going to work in the Bay Area (or perhaps New York City) and very few are going to regulatory agencies in Washington, DC. One solution is for the FDA and other regulatory agencies to set up offices in Silicon Valley.

###  Subproblem 4.2.1 How do we regulate dynamic AIs?

Companies would like to have their AI systems &quot;learn on the fly&quot; and be continually improving. However, right now regulatories agencies (such as the FDA) will only approve one static version of the system, and if it is modified or updated in any way, companies have to go through a long process to get it approved again. More generally, there is a larger technical question about how much we can trust AI systems that are dynamic. This is especially crucial when considering future systems that don&#39;t just retrain weights in model but can also rewrite and expand their own code.

###  Subproblem 4.4. - threat of unsafe / unaligned AGI destroying humanity as a side effect

Policy solution -

- Invest in research on AI Safety. Establish long term funding streams for long term, visionary research in AI safety and making AI aligned with human values. Pursue a &quot;portfolio&quot; of AI safety research including mathematical / decision theory research (such as done at the Machine Intelligence Research Institute), inverse reinforcement learning approaches (such as pursued at Berkeley Center for Human Compatible AI), research on utilitarian ethics (Future of Humanity Institute at Oxford), and finally the BCI approach advocated by Elon Musk.

# Problem 5- AI not progressing fast enough due to technopessimism

We don&#39;t want to be too &quot;doom and gloom&quot; about AI. We need to accelerate AI progress because AI will be essential for figuring out how to stop and reverse aging, target and destroy cancer, and cure all forms of disease. Great economic gains will be had from AI, and we don&#39;t want &quot;bad PR&quot; to slow progress.

# Further Reading:

- [Ben Goertzel&#39;s policy proposal for drastically expanding federal R&amp;D to accelerate technoprogress.](https://ieet.org/index.php/IEET2/more/goertzel20151107)
- [Chapter 15 of David Wood&#39;s new book &quot;RAFT 2035&quot;: &quot;Politics &amp; AI&quot;:](https://transpolitica.org/projects/raft-2035/15-politics-and-ai/)
